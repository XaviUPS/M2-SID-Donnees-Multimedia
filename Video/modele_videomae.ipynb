{"cells":[{"cell_type":"code","source":["#debut de la run\n","\n","import pickle\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","from torchvision import transforms\n","import numpy as np\n","\n","class PKLVideoDataset(Dataset):\n","    def __init__(self, x_files, y_files, extractor, target_frames=16):\n","        self.videos = []\n","        self.labels = []\n","        self.extractor = extractor\n","        self.target_frames = target_frames\n","\n","        for xf, yf in zip(x_files, y_files):\n","            with open(xf, \"rb\") as f:\n","                self.videos.extend(pickle.load(f))\n","            with open(yf, \"rb\") as f:\n","                self.labels.extend(pickle.load(f))\n","\n","    def __len__(self):\n","        return len(self.videos)\n","\n","    def __getitem__(self, idx):\n","        video = self.videos[idx]\n","        label = self.labels[idx]\n","\n","        # Convertir BGR -> RGB\n","        frames = [cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in video]\n","\n","        # Ajuster Ã  target_frames\n","        if len(frames) < self.target_frames:\n","            repeat_factor = int(np.ceil(self.target_frames / len(frames)))\n","            frames = (frames * repeat_factor)[:self.target_frames]\n","        else:\n","            frames = frames[:self.target_frames]\n","\n","        # PrÃ©paration avec lâ€™extractor (gÃ¨re resize, normalization, etc.)\n","        inputs = self.extractor(frames, return_tensors=\"pt\")\n","        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # (C, T, H, W)\n","\n","        return pixel_values, torch.tensor(label, dtype=torch.long)\n","\n","\n","x_files = [\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_train1_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_train2_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_train3_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_train4_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_train5_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_train6_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_train7_small3.pkl\",\n","\n","]\n","y_files = [\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_train1.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_train2.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_train3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_train4.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_train5.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_train6.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_train7.pkl\",\n","\n","]\n","# Fichiers de validation\n","x_val_files = [\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_val1_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_val2_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_val3_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_val4_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_val5_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_val6_small3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/X_val7_small3.pkl\",\n","\n","]\n","y_val_files = [\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_val1.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_val2.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_val3.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_val4.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_val5.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_val6.pkl\",\n","    \"/content/drive/MyDrive/Projet non-alternant/VidÃ©o/ensemble_non_hasard/y_val7.pkl\",\n","\n","]\n","\n","\n","\n","\n","from google.colab import drive\n","\n","# ============================================================\n","# 2ï¸âƒ£ MONTAGE DU DRIVE ET CHARGEMENT DU DATAFRAME\n","# ============================================================\n","drive.mount(\"/content/drive\", force_remount=True)\n","from transformers import VideoMAEFeatureExtractor\n","\n","extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n","\n","train_dataset = PKLVideoDataset(x_files, y_files, extractor=extractor)\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n","\n","val_dataset = PKLVideoDataset(x_val_files, y_val_files, extractor=extractor)\n","val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n"],"metadata":{"id":"W0FOc8g9XsR1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761680944335,"user_tz":-60,"elapsed":100606,"user":{"displayName":"Q Bwj Bwbw","userId":"12243021587343755842"}},"outputId":"a6f37e38-9db6-4652-878f-a0bd5f2ca303"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/transformers/models/videomae/feature_extraction_videomae.py:30: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Lt9rEQ-Fe7Xx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcR-W05F0zJE"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\n","from torch.cuda.amp import autocast, GradScaler\n","\n","# ==========================================\n","# 1ï¸âƒ£ ModÃ¨le et extracteur\n","# ==========================================\n","extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n","model = VideoMAEForVideoClassification.from_pretrained(\n","    \"MCG-NJU/videomae-base\",\n","    num_labels=20\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# ==========================================\n","# 2ï¸âƒ£ Optimiseur, critÃ¨re, scaler\n","# ==========================================\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","scaler = GradScaler()\n","\n","# ==========================================\n","# 3ï¸âƒ£ EntraÃ®nement complet\n","# ==========================================\n","num_epochs = 20\n","best_acc = 0.0\n","model_path = \"videomae_best_model.pth\"\n","\n","train_acc_list, val_acc_list, train_loss_list, val_loss_list = [], [], [], []\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_train_loss, running_train_correct, total_train = 0.0, 0, 0\n","\n","    for pixel_values, labels in tqdm(train_loader, desc=f\"Ã‰poque {epoch+1}/{num_epochs}\"):\n","        pixel_values = pixel_values.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        with autocast():\n","            outputs = model(pixel_values)\n","            loss = criterion(outputs.logits, labels)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        running_train_loss += loss.item()\n","        preds = outputs.logits.argmax(1)\n","        running_train_correct += (preds == labels).sum().item()\n","        total_train += labels.size(0)\n","\n","    train_loss = running_train_loss / len(train_loader)\n","    train_acc = running_train_correct / total_train\n","\n","    # ================= Validation =================\n","    model.eval()\n","    running_val_loss, running_val_correct, total_val = 0.0, 0, 0\n","    with torch.no_grad():\n","        for pixel_values, labels in val_loader:\n","            pixel_values = pixel_values.to(device)\n","            labels = labels.to(device)\n","            with autocast():\n","                outputs = model(pixel_values)\n","                loss = criterion(outputs.logits, labels)\n","            running_val_loss += loss.item()\n","            preds = outputs.logits.argmax(1)\n","            running_val_correct += (preds == labels).sum().item()\n","            total_val += labels.size(0)\n","\n","    val_loss = running_val_loss / len(val_loader)\n","    val_acc = running_val_correct / total_val\n","\n","    train_acc_list.append(train_acc)\n","    val_acc_list.append(val_acc)\n","    train_loss_list.append(train_loss)\n","    val_loss_list.append(val_loss)\n","\n","    print(f\"ðŸ“Š [Ã‰poque {epoch+1}/{num_epochs}] \"\n","          f\"Train Acc: {train_acc:.4f} | Train Loss: {train_loss:.4f} | \"\n","          f\"Val Acc: {val_acc:.4f} | Val Loss: {val_loss:.4f}\")\n","\n","    # ================= Sauvegarde du meilleur modÃ¨le =================\n","    if val_acc > best_acc:\n","        best_acc = val_acc\n","        torch.save(model.state_dict(), model_path)\n","        print(f\"ðŸ’¾ Nouveau meilleur modÃ¨le sauvegardÃ© avec Val Acc = {best_acc:.4f}\")\n","\n","print(\"âœ… EntraÃ®nement terminÃ© !\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}